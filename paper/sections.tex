\begin{abstract}
{\it
\XXX{Lorem ipsum abstract adipiscing dolor amet sit yogurt.}
}
\end{abstract}

\section{Introduction}

File systems that store data persistently are a critical component of
operating systems.
As such, their correctness is fundamental to the integrity and
reliability of operating systems and thus of all higher-level
application software running on those systems.
Data loss and corruption bugs are regularly found in existing
file systems, even widely used ones \cite{yang2006explode},
and loss of persistent data is inherently more serious and has broader
ramifications than kernel bugs that merely require rebooting.
Therefore, proving the functional correctness of file systems is
highly desirable.

Formal verification of software systems is already a
nontrivial task in general\footnote{
``seL4 took
\textit{twenty person-years}!!!''},
but for file systems an additional difficulty arises:
file systems have persistent state on disk, and
that state must remain consistent even in the face of
system crashes.
Strategies like journaling and soft updates are used to keep crashes
from causing corruption and data loss, but in the absence of
verification bugs can, have, and will continue to defeat these
techniques.

To reason about the correctness of a file system one must reason both
about transient in-memory state and persistent on-disk state.
Crashes (arising for example from power failures, hardware faults,
kernel panics in unverified components) erase the transient state.
This includes data in the kernel's buffer cache and, with modern
disks, potentially also unwritten data cached within the disk itself.
This can happen at any time and after any step of execution; unlike
when reasoning about, for example,
interrupts (already difficult enough) it is not possible to
block crashes in order to execute a critical section.

Reasoning about state in the presence of uncontrollable external
interrupts needs
specific support in the program logic.
This support is primarily useful for reasoning about persistent state
in the presence of system crashes, but it can be used for other
things.
For example, one might use it to reason about (transient) program
state in the presence of imprecise floating point exceptions.
% and sometimes blocking signals is too expensive and you have to
% cope with having them come in arbitrarily...

After a crash, the system reboots, and runs a procedure
known as \emph{recovery}, whose goal is to examine the on-disk state
and correct it as necessary to resume normal functioning.
This also requires support in the program logic, because (in general)
the recovery procedure starts with the persistent disk state in an
otherwise illegal intermediate condition that does not meet the
preconditions for any ordinary operation.
The program logic must splice the conditions enforced on the
persistent state during operation (``crash conditions'') to the
precondition of the recovery procedure, and the postcondition of the
recovery procedure (``recovery conditions'') to the preconditions for
normal operations.
The full specification of a file system includes recovery conditions
that describe the guarantees made to applications about the state of
files and other file system objects after a crash occurs.

Chen at al \cite{chen2015using} introduced the concept of \emph{crash
Hoare logic} for reasoning about persistent state in the presence of
arbitrary crashes.
This work introduced the concept of crash conditions, but it does not
have explicit recovery conditions: their file system (FSCQ) is
entirely synchronous (so no data is buffered past an operation
completing) and their program logic does not support concurrency, so
only one operation can be in progress at a time and their recovery
scheme either fully completes or fully aborts it.

While other file systems besides FSCQ have been formally verified as
crash-safe, none to our knowledge support concurrent execution and
many do not support write-back caching.
The ultimate intended goal of this project was to develop a file
system, or at least a file system model, that supports both concurrent
execution and write-back caching, and prove it correct in the presence
of crashes with a relatively strict set of post-crash guarantees.
The actual contribution of this project so far is, first, to extend
crash Hoare logic into \emph{concurrent} crash Hoare logic, to allow
for reasoning about concurrent operations even in the face of crashes,
and second, to take first steps at proving the
logic sound and provide some simple example code demonstrating that
the logic is also useful.
Ultimately, having proven the logic sound, we still intend to use the
concurrent crash Hoare logic to build a fully concurrent file system.

In the remainder of this paper we first discuss related work
(Section \ref{sec:relwork}), then introduce the language and the
small-step operational semantics we use as a basis (Section
\ref{sec:semantics}).
In Section \ref{sec:logic} we describe the program logic.
Then in Section \ref{sec:eval} we discuss our examples and an
evaluation of the usefulness of the logic.
Finally we discuss future work (Section \ref{sec:future}) and in
Section \ref{sec:conclusions} conclude.

\section{Related Work}
\label{sec:relwork}

\subsection{Separation logic}
Separation logic \cite{reynolds2002separation} introduced an intuitive way to
\textit{reason locally} about programs that update heap values.
The program heap is a
mapping from addresses to values.
A judgement on the heap takes the form
$h \vDash P$.
The judgement holds if $h$ satisfies the assertion $P$.
An
example assertion is $10 \mapsto 20$, which asserts that address $10$ contains
(maps to) $20$ (in other words, $h(10) = 20$).

In the classical formulation,
the
assertion $P$ is only true if $h$ is \textit{exactly} the heap described by P,
and nothing more.
(In the intuitionistic formulation,
satisfying $P$ means $h$ is a superset of the heap described by $P$, as opposed
to exactly that heap.
This is useful for reasoning about programs that have
automatic garbage collection as opposed to manual memory management.)

We have a binary separating conjunction operator ($\ast$) and
a unit $\emp$ (which describes the empty heap) with the following
properties\footnote{
Traditional separation logic also has a separating implication operator
($-\!\!\ast$),
% this could be \rightspoon but I don't seem to have the right tex packages
but our logic does not use it.}:

\[
    h \ast \emp \vDash P \imp h \vDash P \\
\]
\begin{align*}
% use \leftrightarrow, not \iff, to match the \imp on the previous line
    h \vDash P \ast R \leftrightarrow \, &h_1 \vDash P \; \wedge \\
		           &h_2 \vDash R \; \wedge \\
		           &h = h_1 \cup h_2 \; \wedge \\
			   &h_1 \bot h_2
\end{align*}
where $h_1 \bot h_2$ is true if and only if $h_1$ and $h_2$ are
disjoint heaps.


Using the separating conjunction, we can create a judgement of the form:

\[ h \vDash (10\mapsto20) * (15\mapsto5) \]

This judgement holds if and only if $h$ is exactly the heap with two allocated
addresses, $10$ pointing to $20$ and $15$ pointing to $5$.

Separation logic extends traditional Hoare logic \cite{hoare1969axiomatic} with
the frame rule:

\[
\infer{\hoaretrip{P * R}{C}{Q * R}}{\hoaretrip{P}{C}{Q}}
\]

Intuitively, if we think of $C$ as a procedure call that has $P$ as its heap
pre-condition and $Q$ as the resulting heap, then we can call $C$ by ``framing
out'' the rest of the heap ($R$) that $C$ doesn't touch, and then framing it
back in after the procedure call.

\subsection{Concurrent Separation Logic}

The ability to reason locally about the heap naturally led to attempting to
reason about shared state in concurrent programs.
This gave rise to concurrent
separation logic \cite{o2007resources}.
Given a binary operator $C_1 ~||~ C_2$ that
runs commands $C_1$ and $C_2$ in parallel, we have the rule:

\[
    \infer{\hoaretrip{P * P'}{C_1 ~||~ C_2}{Q * Q'}}{\hoaretrip{P}{C_1}{Q} &
                                                     \hoaretrip{P'}{C_2}{Q'}}
\]

We can freely run $C_1$ and $C_2$ concurrently if the heaps they use are
disjoint, because they cannot interact with one another.
But this rule alone cannot be used to model
programs that communicate or share state.

The original concurrent separation logic had an atomic
$\mathit{with~} r \mathit{~when~} b \mathit{~do~} c$
control flow operator, and associated logic rule.
This runs command $c$
atomically (using an implicit lock associated with $r$),
spinning until condition $b$ becomes true.
We do not use this model.
Instead, we model locks explicitly, as was done in Hobor et al.'s work on
formulating a concurrent separation logic for C \cite{hobor2008oracle}.
As such, we have rules for both acquiring and
releasing a lock.
Loosely, the rules look like:

\begin{align*}
    \infer{\hoaretrip{\ell \mapsto R}{\texttt{getlock}(\ell)}{(\ell \mapsto R) *
    R}}{} \\[+9px]
    \infer{\hoaretrip{(\ell \mapsto R) * R}{\texttt{putlock}(\ell)}{\ell \mapsto R}}{}
\end{align*}

Here, $R$ represents an assertion (a ``predicate on the heap'').
What exactly it means for a predicate (something meant to be
discussed only by the logic) to live in the heap and be pointed to
is somewhat problematic.
Our logic currently
avoids doing this by not allowing for dynamic lock creation and
deletion.
But
that issue aside, intuitively, acquiring a lock allows us to access new heap
addresses referenced in $R$ that the lock was protecting.

For example, $R$ might look like $\exists x, 10\mapsto x \wedge
\texttt{isEven}(x)$.
After acquiring the lock, we now can read address $10$.
We do not know what $x$ is until we actually perform the read, but we do know
that it is even.
(This is all the information we will ever have when
trying to prove something in the logic, since we cannot directly read the
address to get a concrete value in the logic.)

We can change $x$ arbitrarily while we hold the lock, but when we
release the lock, we must guarantee $x$ is once again even.
Otherwise we do
not meet the pre-condition of putlock, which includes $R$.

\subsection{Other Logics}

In parallel with concurrent separation logic, other methods arose to
reason about concurrent programs.
For example,
rely-guarantee \cite{jones1981development} explicitly models interference
(whereas concurrent separation logic is premised on the assumption that most
programs rarely interfere).
The Views metaframework \cite{dinsdale2013views} is a
general framework that can be used to prove the soundness of various
compositional reasoning strategies for concurrent programs, and has been shown
to generalize both rely-guarantee and concurrent separation logic.

Most similar to our own efforts, Ntzik et al. extend the Views framework to
allow for concurrent fault-tolerant resource reasoning \cite{ntzik2015fault}.
They split pre-conditions and post-conditions into separate volatile
(memory-related) and durable (disk-related) conditions, with separate,
frame-rulable crash invariants.
They use their logic to reason about
ARIES \cite{ntzik2015fault}, a write-ahead logging recovery algorithm.
However,
their logic rules differ from our own \XXX{how? that's the point of
having this here}, and they do not demonstrate an actual
implementation of their logic or proofs in code.
We are interested in the
relationship between our own concurrent crash Hoare logic and their
fault-tolerant resource reasoning.

The Verified Software Toolchain \cite{appel2014program} is an ongoing effort to
allow for functional verification of real C code in Coq that can be immediately
compiled by the CompCert \cite{leroy2009formal} verified C compiler to machine
code.
While their separation logic framework has been used to prove relatively
substantial programs written in C, their fledgling concurrent separation logic
does not yet allow for reasoning about something as complex as a file system.
Our concurrent separation logic does not need to concern itself with the full
semantics of the (supported subset of) the C language, nor does it need to
adhere to any pre-existing semantics as VST must do with CompCert.
Nevertheless,
we were able to reuse VST's underlying
separation algebra machinery in
the implementation of our logic.

In extending a file system to allow for concurrency and buffered writes, we run
into a fundamental issue with concurrent separation logic: proving the
absence of low-level races is relatively easy,
but reasoning about functional correctness becomes difficult.
With multiple concurrent operations and
updates being buffered, the state of the file system before and after
any one operation is no longer readily expressible.
Blom et al. argue that
this problem may be handled with the use of traces \cite{blom2015history}, in a
manner similar to how distributed systems implemented using the Verdi framework
prove functional correctness \cite{wilcox2015verdi}.
This is the approach that
VST is following in the beginnings of their concurrent separation logic, and it
is the one we take in our specifications.

% In this paper, the FSCQ people (+Eddie) discuss approaches, including traces.
%\cite{chen2015specifying}

Crash Hoare logic \cite{chen2015using} is an extension of Hoare logic that, in
addition to the regular pre- and post-conditions, has an additional crash
condition that must hold \textit{at every step} of the execution.
If a given
procedure satisfies some crash condition $C$, then in the event of a crash, we
are guaranteed that $C$ accurately describes the state of the world just before
the crash.
Crash Hoare logic is used in the correctness proofs of the
FSCQ \cite{chen2015using} file system.

\subsection{Verified File Systems}
FSCQ \cite{chen2015using}, written in the Coq proof assistant, is the
first file
system proven to meet a specification that includes crashes, using the
aforementioned crash Hoare logic.
The logic makes use of separation logic to
ensure that procedures modify only the intended data.
Using the separating
conjunction ($\ast$), a specification for the writeblock call can be written
along the lines of:
% couldn't get & to work, resorted to hspace...
\begin{align*}
    \left\{ (\mathit{diskblock}\;b \mapsto \texttt{?}) \ast \mathit{otherblocks} \right\} \\ 
    \texttt{writeblock}(b, 10)\hspace{30px} \\
    \left\{ (\mathit{diskblock}\;b \mapsto 10) \ast \mathit{otherblocks} \right\}
\end{align*}

In the above, the appearance of $\ast ~otherblocks$ on both sides of the Hoare
triple indicates that the write does not touch the other blocks on
disk.
Without
separation logic, a write call could theoretically be allowed to zero out all
other blocks on disk and just not mention that fact in its Hoare triple.

Our work is directly inspired by FSCQ's use of crash Hoare logic, but there are
three issues with FSCQ that we aim to fix.
First, it does not support full
concurrency.
Chajed \cite{chajed2017verifying} extends FSCQ to allow for
asynchronous reads: if a file system call needs to read from disk, it issues the
read and restarts the entire call, allowing another file system call to proceed
while the first waits for the disk read to complete.
However, extending
FSCQ to fully concurrent file system calls seems to require fundamental changes
to its infrastructure.

Second, every write operation to disk requires an explicit disk sync before
returning\footnote{
In fact, it requires four separate syncs.
}.
Writes are not buffered in memory, and the post-condition of a write call would
not be able to state anything about the disk containing new data without a sync.

Finally, related strictly to the logic (and not a restriction on the actual
implementation), the specification does not seem to be able to make full use of
the power of separation logic.
In the specification for \texttt{writeblock}
above, usually separation logic would allow for leaving out $\ast ~otherblocks$
from the pre- and post-conditions, and it would be up to a caller to frame it
out before making the call.
Our logic aims to fix these three concerns.

Other verified file systems suffer from similar deficiencies, especially with
respect to concurrency.
Cogent \cite{amani2016cogent} is a domain-specific
language that certifiably compiles to C, which can be further compiled to
machine code by CompCert.
Proofs in Cogent are at a higher level than low-level
C code, and thus the verification burden is lowered.
Cogent is used to prove
(subsets of) two Linux file systems, BilbyFS and ext2, rewritten in the
language.
However, Cogent's semantics is sequential, and thus while it
supports asynchronous I/O it does not handle concurrency.

Yggdrasil \cite{sigurbjarnarson2016push} is similarly a higher-level toolkit for
implementing file systems based on crash refinement.
It uses Z3 as its underlying
verifier, removing most of the proof burden from the developer.
Again, file
systems implemented using Yggdrasil are limited to single-threaded code.

% Flashix??

\section{Language and Semantics}
\label{sec:semantics}

In order to support concurrency we need a language where executions
can be finely interleaved, and a small-step semantics so that these
interleavings can be formally defined and reasoned about.
In order to mechanize the logic and proofs in Coq this means defining
an abstract syntax and a semantics, and then writing our concurrent
file system code in the abstract syntax.
(By comparison, in FSCQ, the only steps involve updating the disk and
everything in between is pure-functional Coq code.)

Our language \textsc{Chimp} (Concurrent/Crash Hoare \textsc{Imp}) is
basic \textsc{Imp} extended with a minimal set of features necessary
for writing nontrivial concurrent code:
\begin{itemize}
\item procedures and procedure calls/returns;
\item a \texttt{start} operation to fork a new thread;
\item local variables declared in procedures;
\item \texttt{load} and \texttt{store} to access the heap;
\item \texttt{getlock} and \texttt{putlock} to handle locks;
\item and simple explicit types.
\end{itemize}

Local variables are private to the procedures they appear in.
Forking a new thread always invokes a procedure with a new set of
local variables.

Expressions are pure so we can have a big-step semantics for
evaluating them.
This simplifies a number of things greatly.
Both the load and procedure call operations always assign the results
to a variable directly; they are commands and not expressions.

Locks can be thought of as safe handles for global shared objects that
live in the heap; the logic prevents accessing heap variables without
holding the locks that protect them.
This is discussed in Section \ref{sec:logic}.

The disk, or in fact potentially several disks, are treated as
additional heaps and accessed via the same load and store operations
as in-memory variables.
Disk addresses can be protected by locks, but locks may not themselves
live on disks.

We do not currently have condition variables.
As seen in e.g. STM Haskell \cite{stmhaskell}, condition variables are
a performance optimization.
(It might be an interesting exercise to formulate a more sophisticated
abstract machine with condition variables and an explicit concept of
sleeping threads, and prove a refinement.)

The full abstract syntax can be found in Appendix \ref{sec:appendix_ast}.

\subsection{Semantics}

As noted above expressions are pure (they can only access local
variables, which are not shared, and are read-only) so the 
semantics for expression evaluation can be large-step: an expression
just evaluates to a value.
(The supply of expressions we support is somewhat limited, because
originally program values were arbitrary Coq values and thus any Coq
expression could be trivially embedded.)

The rest of the language semantics are written in four layers: one for
ordinary commands/statements (including those that access the heap),
one for the call and return stack, one for threads (which is currently
pass-through but might not be in the future), and one for the complete
machine with threads executing in parallel.
Lock operations happen at the command level, because (at least for
semantic purposes) the lock state, though separate from heap data, is
rolled in with the heap.
This might or might not have been a mistake.
(The semantics of the lock operations have changed repeatedly to suit
the needs of the logic.)
Starting a new thread is effectively calling a procedure in a new
thread context, so it happens mostly at the stack level: a new stack
is created and this is then reflected up to the machine level as a new
thread.
At the machine level, the machine nondeterministically chooses a
thread to run and steps it once.
Acquiring a lock can only progress if the lock is available, so
threads waiting for locks cannot step.
If the system deadlocks, which the logic does not currently attempt to
prevent, the machine will become unable to step.
(That this can be construed as a form of crash...)

The interesting semantic rules can be found in Appendix
\ref{sec:appendix_semantics}.
(The standard ones are omitted to save space.)

\subsection{Typing}

We originally wanted to use arbitrary Coq values as program values,
for a wide range of reasons.
Doing that while not requiring a separate variable environment for
every type requires wrapping up a Coq type and value of that type
inside a value object; extracting these and using the values requires
proving to Coq that the type wrapped up in a value is the type it
ought to be.
This in turn requires a set of soundness propositions on the abstract
syntax.
These propositions were not a type system in the usual sense, but they
were structurally equivalent to one.
Later on we gave up on using arbitrary Coq values; the existing
soundness propositions became very similar ones implementing
a simple type system.

The motivation for worrying about typing, and, indeed, proving
soundness of the typing in terms of preservation and progress, was not
typing per se (largely irrelevant to the project goals) but using the
types as a lever for debugging the semantics.
The soundness of the \emph{logic} is of critical importance; the logic
is sound only with respect to the semantics; and the logic soundness
proofs are far more complicated and harder than type system proofs.
Problems with the semantics that make the logic proofs just not work
can be detected and understood in the context of the type system.
%(For this reason the proofs are only partial -- the completed parts
%are the parts that were either straightforward or interesting in this
%context.)

The experience of updating the typing proofs as the abstract syntax
and semantics have shifted under the demands of the logic has been
valuable in its own right, in a long-term sense.

None of the typing judgments (or other structural soundness
constraints on the abstract syntax) are interesting in themselves, and
so none are exhibited in the Appendix.

% Stuff from Rob's original Language section that isn't (yet?) in the
% text above.
%
% ...
%In their ongoing efforts at a concurrent separation logic, Chen et al.\ have
%added acquiring and releasing locks to the list of steps (but removed sync and
%trim).
% ...
%the write-ahead logging invariant depends on the contents of
%memory as much as it depends on disk (in-memory log entries need to
%be written out to disk before the relevant in-memory buffers)
% ...
%It is possible that,
%strictly for the purposes of verifying a file system like FSCQ, we
%could have gotten away with just adding heap manipulation and lock
%operations, performing everything in between in pure Coq.
%   - Rob
%Maybe; you'd have to treat the local environment as a monad. It
%might work. Far from clear if it would be preferable, either as
%a way of writing things down or as a representation for reasoning
%about them. Monad code basically unfolds to continuations and
%continuations suck.
%   - David

\section{Crash Concurrent Separation Hoare Logic}
\label{sec:logic}

\subsection{Semantics}

\subsection{Logic}

\subsubsection{Notation}
Because the language is imperative, we are able to take some inspiration from
the Verified Software Toolchain for the Hoare logic rules.
On the one hand, we
are able to simplify, since we do not have to support the broader C semantics
that VST does.
On the other, we need to extend the logic to support both
concurrency and crash conditions.

A Hoare judgement for a statement of our language is a septuple of the form:

$$\hoarestmt{(R^c,R^r)}{(L^c,L^r)}{CP}{P}{s}{Q}{CQ}$$

$P$ and $Q$ are the pre- and post-conditions, respectively, of the statement
$s$, as in traditional Hoare logic.
$CP$ and $CQ$, on the other hand, are the
crash pre-condition and crash post-condition.
\footnote{Calling it the ``crash
post-condition'' seems to imply that that is the state of the world after a
crash, but that is not right.
Nevertheless, its what we have become accustomed
to calling it, as it is analogous to FSCQ's single crash condition.}
The crash
pre-condition and crash post-condition are really statements about invariants.
The crash pre-condition states that every step taken in $s$ respects the
invariants specified in $CP$.
The crash post-condition states that, after $s$
completes, future steps of the program must continue to abide by the invariants
in $CQ$.
Importantly, $CP$ and $CQ$ \textit{do not} have to be the same; $s$
can modify them.
$(L^c,L^r)$ and $(R^c,R^r)$ are described in the Lock rules
and Return rule subsection, respectively.
We will denote them with the
contracted notation $L$ and $R$ when the components are not being discussed.

\subsubsection{Lock rules}

How can a statement take in an invariant that is supposed to hold at every step
of execution, and ultimately require a \textit{different} invariant after it
executes? Without any additional infrastructure, it makes sense that $s$ can add
further invariants to $CP$, such that $CQ$ is ultimately a superset of $CP$.
However, the logic can only scalably reason about large programs if we can
take invariants \textit{away}.

As described in regular concurrent separation logic, acquiring a lock can pull a
resource invariant into the pre-condition of a statement, and releasing a lock
can pull a resource invariant out of the post-condition.
Analogously, we model
lock acquisition to pull a separate resource invariant into the crash
pre-condition and lock release to pull it out of the crash post-condition.

We call the resource invariant pulled into the regular pre- and post-conditions
a \textit{weak invariant}.
The invariant only needs to hold at the time the lock
is acquired and when it is released.
In between lock acquisition and release,
the invariant may be violated.
We call the (possibly different) resource
invariant pulled into the crash pre- and post-conditions a \textit{strong
invariant}.
The invariant must be true at the time the lock is acquired; it must
be true at the time the lock is released; and it must remain true at each step
of execution in between the acquisition and release.

In the Hoare septuple above, $L^c$ is a map from a lock to its associated strong
invariant that is pulled into and out of the crash pre-condition and
post-condition, and $L^r$ is the traditional map from a lock to its associated
weak invariant that is pulled into and out of the regular pre-condition and
post-condition.
Given all this, the rules for lock acquisition and release
become what is written in Figure~\ref{fig:locks}.
\footnote{Notice that we avoid
having to discuss the storage of invariants in the heap.
Instead, analogously
to locks in C, we just need to have a pointer to the lock in our heap in order
to acquire and release it.}

\begin{figure*}
\begin{gather*}
    \forall a, \infer{\hoarestmt{R}{(L^c,L^r)}{a \mapsto \ell}{a \mapsto \ell}{\texttt{getlock}(\ell)}{a \mapsto \ell *
    L^r(\ell)}{a \mapsto \ell * L^c(\ell)}}{} \\[+9px]
    \forall a, \infer{\hoarestmt{R}{(L^c,L^r)}{a \mapsto \ell * L^c(\ell)}{a \mapsto \ell *
    L^r(\ell)}{\texttt{putlock}(\ell)}{a \mapsto \ell}{emp}}{}
\end{gather*}
\caption{Rules for getlock and putlock}
\label{fig:locks}
\end{figure*}

\subsubsection{Frame rules}

What happens if we try to acquire two completely independent locks, $\ell_1$ and
$\ell_2$, in sequence? Unfortunately, when we try to acquire $\ell_2$, we could
have a non-empty crash pre-condition arising from $L^c(\ell_1)$, and as such
would not meet the crash pre-condition of $\texttt{getlock}(\ell_2)$.

We would like to use a frame rule to ``frame out'' $L^c(\ell_1)$ from the crash
pre- and post-conditions before calling $\texttt{getlock}(\ell_2)$.
Unfortunately, it cannot be as general as the original frame rule of separation
logic: here, to frame constraints out of the crash pre- and post-conditions, we
must also frame knowledge of the constrained portions of the heap out of the
regular pre- and post-conditions.
Otherwise, we could frame out a strong
invariant and retain the ability to modify the heap in a way that violates the
strong invariant.
As such, the additional frame rule carries some restrictions,
as seen in Figure~\ref{fig:stronginv}.

\begin{figure*}
\begin{gather*}
    \forall a, \infer
	{\hoarestmt{R}{(L^c,L^r)}{CP * L^c(\ell)}{P * a \mapsto \ell * L^r(\ell)}{s}{Q * a \mapsto \ell * L^r(\ell)}{CQ * L^c(\ell)}}
	{\hoarestmt{R}{(L^c,L^r)}{CP}{P}{s}{Q}{CQ}}
\end{gather*}
\caption{Frame rule for strong invariants}
\label{fig:stronginv}
\end{figure*}

Intuitively, if statement $s$ does not rely in any way whatsoever on lock $\ell$
in order to execute, then we can completely frame everything related to it out
of existence (including the lock itself).
$s$ cannot possibly violate constraint
$L^c(\ell)$, because everything that the acquisition of $\ell$ pulled in has
been framed out.
\footnote{Note that this implies a constraint on the possible
dynamic crash conditions that we can support: a crash condition should
\textit{only} constrain the use of the resource that the lock
protects.
It does
not make sense for the acquisition of a lock to insert a strong invariant about
some global variable not protected by that lock, since any thread is in danger
of breaking that invariant when not holding the lock.
Furthermore, it is
frustrating that we have to specify the mapping of the lock in the crash
conditions.
This would appear to imply that this must \textit{always} be the
mapping of the lock.
We could just indicate that there exists some mapping
for the lock, but that would still imply that the lock is always allocated.
This is fine in our current world where we do not have dynamic memory or lock
operations, but is an issue once those are added into the language.
See the
future work section for a discussion on our proposed solution to this issue.}
% another wording than the ``mapping of the lock'' perhaps

A more granular rule is expected to remain sound.
A strong invariant cannot be
violated as long as the resource it constrains is removed from the weak
invariant.
Then, as long as we simultaneously frame the same referenced
addresses and all constraints on those addresses out of both the crash and
regular conditions, we do not need to frame \textit{everything} related to the
lock out at once.
With this, we can combine the two frame rules into one, as
seen in Figure~\ref{fig:altframe}.

%If a mapping is only
%mentioned in the weak invariant $L^r(\ell)$, we should not need to frame it out
%of the regular conditions in order to remove $L^c(\ell)$ from the crash
%conditions.
%Furthermore, when framing out $L^c(\ell)$, any resource mentioned in
%both $L^c(\ell)$ and $L^r(\ell)$ can just be downgraded to \textit{read-only} in
%$L^r(\ell)$, so that the resource cannot be changed to violate the constraint.
%With this, we can combine the two frame rules into one, as seen in
%Figure~\ref{fig:altframe}.
%\footnote{We use the notation seen in permission
%separation logics, where $a\overset{\pi}{\mapsto}\texttt{?}$ indicates a
%partial, read-only permission to address $a$, and
%$a\overset{\bullet}{\mapsto}\texttt{?}$ indicates a full, write permission to
%address $a$.}

There is one important thing that the weaker (more granular) rule handles that
the stronger rule cannot.
Consider an example of a lock $\ell_1$ protecting a
structure that contains another lock $\ell_2$ (for example, in a linked list
hand-over-hand lock coupling program).
The crash pre-condition of
$\texttt{getlock}(\ell_2)$ requires everything related to $\ell_1$ to have been
framed out.
But the mapping $a_2\mapsto\ell_2$, which is a part of the
statement's pre-condition, is itself a resource in the invariants of $\ell_1$!
The weaker frame rule would allow for removal of all \textit{other} resources
and constraints, which still allows for acquisition of the lock.

We intend to switch to (a form of) the alternative frame rule, but we currently
have implemented the two separate frame rules.
See the future work discussion
for why the frame rule (and more fundamentally the shape of our Hoare triple)
might still change further, whereas we expect (but have not yet proven) that
our two current frame rules are sound.

\begin{figure*}
\begin{gather*}
    \infer[\forall a, (a \mapsto\texttt{?}) \in CF \imp
    (a\mapsto\texttt{?}) \notin P]
	{\hoarestmt{R}{L}{CP * CF}{P * F}{s}{Q * F}{CQ * CF}}
	{\hoarestmt{R}{L}{CP}{P}{s}{Q}{CQ}}
\end{gather*}
\caption{Alternative frame rule}
\label{fig:altframe}
\end{figure*}

%However, the current rule makes sense from a design
%perspective.
%With this rule, all procedures must be explicit about which locks
%they ever use, and analyzing the uld generally suffice.
%If a mapping is only
%mentioned in the weak invariant $L^r(\ell)$, we should not need to remove it
%from the regular pre-condition in order to remove $L^c(\ell)$ from the crash
%conditions.
%However, the current rule makes sense from a design
%perspective.
%With this rule, all procedures must be explicit about which locks
%they will ever use (which includes just touching resources belonging to the
%locks), whether they expect them to be held or not held at the time of
%acquisition, and whether they return with the lock held or not
%held.
%\footnote{Technically, we can only tell these things if the lock has an
%associated strong invariant.
%However, the pre-condition can still be written
%explicitly in terms of $L^c(\ell)$, even if that evaluates to $emp$, which
%indicates whether the procedure expects the lock to be held.
%Similarly, we
%could force every lock to automatically have \textit{some} strong invariant,
%perhaps related to the mapping $a\mapsto\ell$}
%
%Also, notice that the logic will not let us acquire a lock twice, unlike regular
%concurrent separation logic.
%\footnote{As in the above, this currently depends on
%a strong lock invariant existing.}

\subsubsection{Rule of consequence}

% XXX return septuple
% XXX strengthening invariants dynamically

Ignoring separation logic and crash post-conditions for a moment, if we
currently have crash pre-condition $CP'=A \wedge B$, under what circumstances
can we execute a statement $s$ with pre-condition $CP$? We can clearly do so if
$CP=CP'$.
What if $CP=A$? Then statement $s$ does not adhere to invariant $B$,
and we cannot execute it.
On the other hand, if $CP=A \wedge B \wedge C$, then
$s$ adheres to a superset of the invariants that we need, and we can
proceed.
In
general, we can execute $s$ if $CP \imp CP'$.

Now focusing on crash post-conditions, we have crash post-condition $Q'=A \wedge
B$, and statement $s$ has crash post-condition $Q$.
If $Q=A \wedge B \wedge C$,
then $s$ requires that future steps adhere to $C$, which we do not promise to
adhere to.
On the other hand, if $Q=A$, then $s$ requires that all future steps
adhere only to $A$, which is a subset of what we already intended to adhere to
after this step.
In general, we can execute $s$ if $CQ \imp CQ'$.

The full rule of consequence, combined with that of traditional Hoare logic, is
as shown in Figure~\ref{fig:consequence}.
Interestingly, the implications in
the crash rules are the \textit{converse} of the implications in the traditional
rules!

\begin{figure*}
\begin{gather*}
\infer{\hoarestmt{R}{L}{CP'}{P'}{s}{Q'}{CQ'}}
      {CP \imp CP' & P' \imp P & \hoarestmt{R}{L}{CP}{P}{s}{Q}{CQ}
      & Q \imp Q' & CQ' \imp CQ}
\end{gather*}
\caption{Rule of Consequence}
\label{fig:consequence}
\end{figure*}

In traditional Hoare logic, a Hoare triple cannot be established for a sequence
where the second statement has a pre-condition of False (unless the first
statement was able to establish a post-condition of False).
Similarly, having
proven a Hoare triple with a post-condition of False means we can sequence with
a statement that has any other pre-condition (and thus prove anything), because
we can use the rule of consequence to change the post-condition to match
whatever pre-condition we like.

What does it mean for a statement's crash pre-condition to be True? By the rule
of consequence, it means we cannot establish a Hoare septuple for a sequence
to this statement (unless the current crash post-condition happens to also be
True).
Similarly, a crash pre-condition of True may be established for any
statement, since any statement's more precise crash pre-condition may imply
True.
Intuitively, a True crash pre-condition releases the Hoare septuple's
statement from adhering to \textit{any} invariants; a Hoare septuple with a
True crash pre-condition does not claim the statement adheres to any invariant.

By similar reasoning, if a statement's crash pre-condition is False, then it
supports \textit{all} possible invariants.
For example, an operation that does
not modify any state can have a crash pre-condition of False.

If a statement's crash pre-condition is $emp$, then that asserts and requires
that its heap is permanently empty (the only way we could add something to the
heap is by acquiring a lock, which would require having access to something on
the heap in the first place).
This could be useful for specifying completely
pure functions that do not touch the heap.

On the other side of the septuple, what about the crash post-condition
(assuming we meet the pre-conditions)? If a statement's crash post-condition
is True, then we can sequence to a statement with any crash pre-condition.
Again, True implies respect for no invariant, and so if the statement ends
without further requirements for respected invariants, we can just add more
invariants to get the invariant that the sequenced statement promises to
respect.

On the other hand, if the statement's crash post-condition is False, we can
never execute another statement (unless the next statement's crash
post-condition is also False).

Finally, if the statement's crash post-condition is $emp$, the heap must remain
empty after the statement returns.
This is not strictly permanent, though,
since we could always have framed out part of the heap prior to executing the
statement, and frame it back in after.

Interestingly, even applying the rule of consequence seems to work in the
opposite direction for crash conditions.
Take the rule for the \texttt{skip}
statement as an example:

\begin{gather*}
    \infer{\hoarestmt{R}{L}{CP}{P}{\texttt{skip}}{P}{CP}}{}
\end{gather*}

If the current state of our judgement looks like:

\begin{gather*}
\infer{\hoarestmt{R}{L}{True}{False}{\texttt{skip}}{Q}{CQ}}{}
\end{gather*}

(which is a very possible state after applying the Return Rule, described in the
next subsection), how can we use the skip rule to prove this judgement? As
described earlier, we can use a crash pre-condition of True to reach any crash
condition, and a regular pre-condition of False to reach any regular condition.

In this case, we can weaken our regular pre-condtion from False to $Q$
using the rule of consequence.
But to make the crash conditions match, we go in
the opposite direction.
We use the rule of consequence on the crash
\textit{post-condition} to bring the state from $CQ$ to True.
Now we can apply
the skip rule with $P=Q$ and $CP=True$, and we are all set.

\subsubsection{Procedure rules}

In our language, a statement can call another procedure and can return from
within a procedure.
Ultimately, a procedure judgement is of roughly the form:
$$\hoareproc{L}{CP}{P}{\texttt{proc}(s, e)}{Q}{CQ}$$
This is to say, a procedure with body $s$ with takes $e$ as an argument must
satisfy the specified pre- and post-conditions given the (global) lock
invariants.
Notably missing is the return condition, $R$, found in the statement
judgements.
In order to prove the procedure judgement sound, we just need to
prove that statement $s$ satisfies:
\begin{gather*}
\hoarestmt{(CQ(e),Q(e))}{L}{CP(e)}{P(e)}{s}{False}{True}
\end{gather*}

First, the procedure's conditions are all parameterized by the input argument.
Second, the procedure's post-conditions are pulled into the \textit{return}
conditions of the body, the post-condition is False, and the post
crash-condition is True.
False and True enforce that the body must always have a
return statement, which, as we will see, is a rule that can satisfy those
constraints (after hitting a return, we should be able to trivially prove the
rest of the procedure, which is often just ``skip'').

The return rule takes the form:

\begin{gather*}
    \infer{\hoarestmt{(R^c,R^r)}{L}{CP}{P}{\texttt{return}(e)}{False}{True}}
	  {R^c(e) \imp CP & P \imp R^r(e)}
\end{gather*}

$(R^c,R^r)$ came directly from the crash and regular post-conditions of the
procedure.
They were already parameterized by the input argument, and are now
further parameterized by the return value (such that the input argument and the
return value can be related in the pre- and post-conditions for the procedure).
If the current crash condition implies the procedure's crash post-condition and
the current regular condition implies the procedure's post-condition (again, the
crash condition is the reverse of the regular condition), then the judgement is
sound.

If the body of the procedure is proven sound according to the procedure's pre-
and post-conditions, then in proving the correctness of any other procedure that calls
the proven procedure, assuming that the current pre-conditions
match those of the proven procedure with the input argument passed in,
we can simply drop in the procedure's post-conditions, parameterized with both
the input argument and the return value.

\subsubsection{Crash-preserving rules}

% XXX is disk model changing by removing the bool?

Ultimately, the only statement of our language whose corresponding hoare rule
needs to be checked against the current crash condition is ``store''.
In our
model, the disk is represented as a parallel version of the heap that can
ultimately hold arbitrary values.
Disk asynchronicity can be represented, for
example, by storing lists at each disk address, where the head of the list gives
the most recent value written to disk, and the tail represents potential values
that could appear on disk following a crash if the disk is not explicitly synced.
The list can be trimmed to the single head element on a disk sync.

On a store, in addition to the regular concurrent separation logic rules
requiring that the address is currently mapped in our heaplet, we just need to
be sure that the new value being stored at the heap address does not violate any
strong invariants in our current crash condition associated with that heap
address.

\section{Example and Evaluation}
\label{sec:eval}

Our logic has been formalized in the Coq proof assistant.

% usefulness
% useability

\section{Future Work}
\label{sec:future}

% XXX Lemma about strong invariants always holding??

The final goal is to use our concurrent crash Hoare logic to prove data-race
freedom and functional correctness of a concurrent crash file system.
But first,
we still need to fully prove the logic sound according to the semantics of our
imperative language.

That said, our logic and language must be extended in multiple ways to more
fully model a concurrent file system.
First, it is necessary to add in a
permission model to allow for multiple readers of a heap address.
For file
system purposes, this is strictly necessary in order to reason about multiple
concurrent threads sharing pointers to the same locks.

Additionally, our language does not currently include any dynamic heap
allocation, including malloc, free, lock creation, and lock deletion.
These are
necessary to be able to correctly model operations such as file system mount and
unmount.
A fundamental issue is that, traditionally, dynamic locks introduce the
aforementioned issue of ``predicates on the heap''.
Once locks are dynamic and
predicates are placed on the heap, we lose the power to reason about what global
invariants a program must abide by, and as such can no longer reason about a
consistent state for recovery to act upon.

We have toyed with the idea that a solution to this problem is to give each lock
an explicit type, and all locks of a given type must share the same resource
invariant (parameterized by the address of the resource that the lock is
protecting).
That way, we no longer have predicates on the heap, and the
$(L^c,L^r)$ mapping to the left of $\vDash$, parameterized by both lock type and
address, once again contains all possible strong invariants. Here, the strong
invariants associated with each lock type would constitute the global invariants
the program adheres to.

As mentioned earlier, there is an alternative frame rule that combines the two
frame rules that we currently have that we have not yet implemented.
A related
issue is that any resource pulled into the pre-condition must also be pulled
into the crash pre-condition, though the actual constraints (weak vs.\ strong)
can differ between the two.
Once dynamic memory is implemented, it seems to make
even less sense for something like $\exists (x:vnode), 10\mapsto x$ to appear in
a strong invariant, since $10$ can be deallocated and reallocated.

Our proposed solution to this problem is to combine the crash conditions into
the regular conditions and maintain only a set of addresses, each of which
would be tagged directly with both the weak and strong invariants by which it
must abide (according to the lock type that protects the address).
The strong
invariant must be respected whenever we store
to that address, and the weak invariant must be respected whenever we attempt to
release the lock protecting that address.
Now, we have an even simpler frame
rule that can frame anything out of the current state, since framing out an
address automatically frames out both the strong and weak invariants associated
with it.

As an aside, it might have been possible to avoid making the language completely
imperative by just extending FSCQ's disk read and write operations with lock and
heap operations.
It was only recently we realized it made sense to restrict
crash resource invariants to only describe precisely the resource that the lock
protects.
The fundamental concurrent crash Hoare logic remains the same, but we
could potentially use Coq values directly.

\section{Conclusions}
\label{sec:conclusions}

\appendix
\section{Language}
\label{sec:appendix_ast}
\section{Semantics}
\label{sec:appendix_semantics}
\section{Proof Rules}
\label{sec:appendix_logic}
